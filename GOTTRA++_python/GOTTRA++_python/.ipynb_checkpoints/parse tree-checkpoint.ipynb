{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sebastian Thrun PERSON\n",
      "Google ORG\n",
      "2007 DATE\n",
      "American NORP\n",
      "Thrun PERSON\n",
      "Recode ORG\n",
      "earlier this week DATE\n",
      "my fries were super gross such disgusting fries 0.7139702518721635\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Process whole documents\n",
    "text = (u\"When Sebastian Thrun started working on self-driving cars at \"\n",
    "        u\"Google in 2007, few people outside of the company took him \"\n",
    "        u\"seriously. “I can tell you very senior CEOs of major American \"\n",
    "        u\"car companies would shake my hand and turn away because I wasn’t \"\n",
    "        u\"worth talking to,” said Thrun, now the co-founder and CEO of \"\n",
    "        u\"online higher education startup Udacity, in an interview with \"\n",
    "        u\"Recode earlier this week.\")\n",
    "doc = nlp(text)\n",
    "\n",
    "# Find named entities, phrases and concepts\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)\n",
    "\n",
    "# Determine semantic similarities\n",
    "doc1 = nlp(u\"my fries were super gross\")\n",
    "doc2 = nlp(u\"such disgusting fries\")\n",
    "similarity = doc1.similarity(doc2)\n",
    "print(doc1.text, doc2.text, similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Un det utilisateur NOUN []\n",
      "utilisateur ROOT utilisateur NOUN [Un, clique, bouton, .]\n",
      "clique amod utilisateur NOUN []\n",
      "sur case bouton NOUN []\n",
      "le det bouton NOUN []\n",
      "bouton nmod utilisateur NOUN [sur, le]\n",
      ". punct utilisateur NOUN []\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "#from spacy.lang.fr.examples import sentences\n",
    "#fr_default\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "doc = nlp(u\"Un utilisateur clique sur le bouton.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "          [child for child in token.children])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il nsubj clique VERB []\n",
      "clique ROOT clique VERB [Il, bouton, .]\n",
      "sur case bouton NOUN []\n",
      "le det bouton NOUN []\n",
      "bouton obl clique VERB [sur, le]\n",
      ". punct clique VERB []\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('fr_core_news_sm')\n",
    "doc = nlp(u\"Il clique sur le bouton.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "          [child for child in token.children])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "def unique_words(lines):\n",
    "    return set(chain(*(line.split() for line in lines if line)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3967\n"
     ]
    }
   ],
   "source": [
    "with open('Ressources/specs.txt', 'r') as f:\n",
    "    print(len(unique_words(f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2488\n"
     ]
    }
   ],
   "source": [
    "with open('Ressources/specs_sans_stopwords.txt', 'r') as f:\n",
    "    print(len(unique_words(f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2381, 165)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"Ressources/data_action_verbs_only/all_data_with_action_verbs.csv\", encoding=\"ISO-8859-1\", sep=\",\")\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import treetaggerwrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = treetaggerwrapper.TreeTagger(TAGLANG='fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abbrevfile': None,\n",
       " 'abbterms': {},\n",
       " 'chunkerproc': None,\n",
       " 'dummysequence': 'Cela\\nest\\nune\\nphrase\\ninutile\\npour\\nassurer\\nla\\ntransmission\\ndes\\ndonnées\\n.',\n",
       " 'fchar': '!?¿;\"«»“”´`¨,*¤@°:%|¦/()[\\\\]{}<>«»\\x8b\\x9b\\x93&~=±×\\x96\\x97\\t\\n\\r—‾̅‒–£¥$€©®\\'',\n",
       " 'fchar_re': re.compile(r'^(.*)([!?¿;\"«»“”´`¨,*¤@°:%|¦/()[\\]{}<>«»\\x8b\\x9b\\x93&~=±×\\x96\\x97\\t\\n\\r—‾̅‒–£¥$€©®\\'])$',\n",
       " re.IGNORECASE|re.UNICODE|re.VERBOSE),\n",
       " 'fcharandperiod_re': re.compile(r'(.*)([!?¿;\"«»“”´`¨,*¤@°:%|¦/()[\\]{}<>«»\\x8b\\x9b\\x93&~=±×\\x96\\x97\\t\\n\\r—‾̅‒–£¥$€©®\\'.])\\.$',\n",
       " re.UNICODE),\n",
       " 'fclictic': \"'-t-elles|-t-ils|-t-on|-ce|-elles|-ils|-je|-la|-les|-leur|-lui|-mêmes|-memes|-même|-meme|-m'|-moi|-on|-toi|-tu|-t'|-vous|-en|-y|-ci|-là|-la\",\n",
       " 'fclictic_re': re.compile(r\"^(.*)('-t-elles|-t-ils|-t-on|-ce|-elles|-ils|-je|-la|-les|-leur|-lui|-mêmes|-memes|-même|-meme|-m'|-moi|-on|-toi|-tu|-t'|-vous|-en|-y|-ci|-là|-la)$\",\n",
       " re.IGNORECASE|re.UNICODE|re.VERBOSE),\n",
       " 'lang': 'fr',\n",
       " 'langsupport': {'abbrevfile': 'french-abbreviations-utf8',\n",
       "  'dummysentence': 'Cela est une phrase inutile pour assurer la transmission des données .',\n",
       "  'encoding': 'utf-8',\n",
       "  'fchar': '!?¿;\"«»“”´`¨,*¤@°:%|¦/()[\\\\]{}<>«»\\x8b\\x9b\\x93&~=±×\\x96\\x97\\t\\n\\r—‾̅‒–£¥$€©®\\'',\n",
       "  'fclictic': \"'-t-elles|-t-ils|-t-on|-ce|-elles|-ils|-je|-la|-les|-leur|-lui|-mêmes|-memes|-même|-meme|-m'|-moi|-on|-toi|-tu|-t'|-vous|-en|-y|-ci|-là|-la\",\n",
       "  'number': '(\\n                    [-+]?[0-9]+(?:[.,][0-9]*)?(?:[eE][-+]?[0-9]+)?\\n                        |\\n                    [-+]?[.,][0-9]+(?:[eE][-+]?[0-9]+)?\\n                )',\n",
       "  'pchar': '!?¿;\"«»“”´`¨,*¤@°:%|¦/()[\\\\]{}<>«»\\x8b\\x9b\\x93&~=±×\\x96\\x97\\t\\n\\r—‾̅‒–£¥$€©®\\'',\n",
       "  'pclictic': \"[dcjlmnstDCJLNMST]'|[Qq]u'|[Jj]usqu'|[Ll]orsqu'\",\n",
       "  'repldnsexp': 'dns-remplacé',\n",
       "  'replemailexp': 'email-remplacé',\n",
       "  'replipexp': 'ip-remplacée>',\n",
       "  'replurlexp': 'url-remplacée',\n",
       "  'tagparfile': 'french-utf8.par'},\n",
       " 'number': '(\\n                    [-+]?[0-9]+(?:[.,][0-9]*)?(?:[eE][-+]?[0-9]+)?\\n                        |\\n                    [-+]?[.,][0-9]+(?:[eE][-+]?[0-9]+)?\\n                )',\n",
       " 'number_re': re.compile(r'(\\n                    [-+]?[0-9]+(?:[.,][0-9]*)?(?:[eE][-+]?[0-9]+)?\\n                        |\\n                    [-+]?[.,][0-9]+(?:[eE][-+]?[0-9]+)?\\n                )',\n",
       " re.IGNORECASE|re.UNICODE|re.VERBOSE),\n",
       " 'pchar': '!?¿;\"«»“”´`¨,*¤@°:%|¦/()[\\\\]{}<>«»\\x8b\\x9b\\x93&~=±×\\x96\\x97\\t\\n\\r—‾̅‒–£¥$€©®\\'',\n",
       " 'pchar_re': re.compile(r'^([!?¿;\"«»“”´`¨,*¤@°:%|¦/()[\\]{}<>«»\\x8b\\x9b\\x93&~=±×\\x96\\x97\\t\\n\\r—‾̅‒–£¥$€©®\\'])(.*)$',\n",
       " re.IGNORECASE|re.UNICODE|re.VERBOSE),\n",
       " 'pclictic': \"[dcjlmnstDCJLNMST]'|[Qq]u'|[Jj]usqu'|[Ll]orsqu'\",\n",
       " 'pclictic_re': re.compile(r\"^([dcjlmnstDCJLNMST]'|[Qq]u'|[Jj]usqu'|[Ll]orsqu')(.*)$\",\n",
       " re.IGNORECASE|re.UNICODE|re.VERBOSE),\n",
       " 'removesgml': False,\n",
       " 'repldnsexp': 'dns-remplacé',\n",
       " 'replemailexp': 'email-remplacé',\n",
       " 'replipexp': 'ip-remplacée>',\n",
       " 'replurlexp': 'url-remplacée',\n",
       " 'tagbin': 'C:\\\\TreeTagger\\\\bin\\\\tree-tagger.exe',\n",
       " 'tagbindir': 'C:\\\\TreeTagger\\\\bin',\n",
       " 'tagdir': 'C:\\\\TreeTagger',\n",
       " 'taggerlock': <unlocked _thread.lock object at 0x00000000058E3CD8>,\n",
       " 'taginencerr': 'replace',\n",
       " 'taginencoding': 'utf-8',\n",
       " 'taginput': None,\n",
       " 'taglibdir': 'C:\\\\TreeTagger\\\\lib',\n",
       " 'tagopt': '-token -lemma -sgml -quiet -no-unknown',\n",
       " 'tagoutencerr': 'replace',\n",
       " 'tagoutencoding': 'utf-8',\n",
       " 'tagoutput': None,\n",
       " 'tagparfile': 'C:\\\\TreeTagger\\\\lib\\\\french-utf8.par',\n",
       " 'tagpopen': None}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint \n",
    "\n",
    "vars(tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
